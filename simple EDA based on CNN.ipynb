{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c9cb39",
   "metadata": {},
   "source": [
    "# 1.Base model. \n",
    "60 seconds x11 features, processed by CNN, parquet uploaded from binance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fb462a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-03.csv.parquet 2022-12-18 08:50:34.579234\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-03.csv.parquet 15000  frames per minute = 14989.2\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-03.csv.parquet 30000  frames per minute = 15929.3\n",
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-04.csv.parquet 2022-12-18 08:52:34.998542\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-04.csv.parquet 45000  frames per minute = 15720.7\n",
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 2022-12-18 08:54:07.576345\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 60000  frames per minute = 15315.5\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 75000  frames per minute = 15779.5\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 90000  frames per minute = 16098.4\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 105000  frames per minute = 16371.1\n",
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 2022-12-18 08:57:12.465024\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 120000  frames per minute = 15875.0\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 135000  frames per minute = 16093.6\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 150000  frames per minute = 16252.8\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 165000  frames per minute = 16387.8\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 180000  frames per minute = 16499.6\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 195000  frames per minute = 16609.1\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 210000  frames per minute = 16698.5\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 225000  frames per minute = 16785.8\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 240000  frames per minute = 16874.2\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 255000  frames per minute = 16949.1\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 270000  frames per minute = 17014.6\n",
      "final d3 271626 positive 20658\n",
      "['open', 'high', 'low', 'close', 'volume', 'openv', 'highv', 'lowv', 'closev', 'C2', 'S2', 'C1', 'S1', 'rsi', 'atr', 'macd', 'returns', 'to_mean', 'to_largest', 'to_smallest']\n",
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 2022-12-18 09:06:37.225716\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 15000  frames per minute = 871.1\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 30000  frames per minute = 1663.0\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 45000  frames per minute = 2385.8\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 60000  frames per minute = 3047.3\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 75000  frames per minute = 3656.9\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 90000  frames per minute = 4217.5\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 105000  frames per minute = 4736.8\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 120000  frames per minute = 5219.5\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 135000  frames per minute = 5669.2\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 150000  frames per minute = 6087.4\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 165000  frames per minute = 6478.2\n",
      "final d3 176971 positive 7543\n",
      "['open', 'high', 'low', 'close', 'volume', 'openv', 'highv', 'lowv', 'closev', 'C2', 'S2', 'C1', 'S1', 'rsi', 'atr', 'macd', 'returns', 'to_mean', 'to_largest', 'to_smallest']\n",
      "Epoch 1/20\n",
      "2717/2717 [==============================] - 191s 70ms/step - loss: 0.7910 - auc: 0.7398 - val_loss: 0.2269 - val_auc: 0.7541 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2717/2717 [==============================] - 183s 67ms/step - loss: 0.2351 - auc: 0.8169 - val_loss: 0.2192 - val_auc: 0.7518 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "2717/2717 [==============================] - 188s 69ms/step - loss: 0.2324 - auc: 0.8209 - val_loss: 0.2293 - val_auc: 0.7700 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "2717/2717 [==============================] - 191s 70ms/step - loss: 0.2309 - auc: 0.8236 - val_loss: 0.2264 - val_auc: 0.7637 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "2717/2717 [==============================] - 191s 70ms/step - loss: 0.2299 - auc: 0.8255 - val_loss: 0.2178 - val_auc: 0.7745 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "2717/2717 [==============================] - 185s 68ms/step - loss: 0.2284 - auc: 0.8268 - val_loss: 0.2144 - val_auc: 0.7753 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "2717/2717 [==============================] - 189s 69ms/step - loss: 0.2281 - auc: 0.8269 - val_loss: 0.2115 - val_auc: 0.7785 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "2717/2717 [==============================] - 185s 68ms/step - loss: 0.2279 - auc: 0.8272 - val_loss: 0.2121 - val_auc: 0.7783 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "2717/2717 [==============================] - 188s 69ms/step - loss: 0.2271 - auc: 0.8282 - val_loss: 0.2168 - val_auc: 0.7745 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "2717/2717 [==============================] - 188s 69ms/step - loss: 0.2267 - auc: 0.8298 - val_loss: 0.2181 - val_auc: 0.7759 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "2717/2717 [==============================] - 178s 65ms/step - loss: 0.2244 - auc: 0.8334 - val_loss: 0.2279 - val_auc: 0.7705 - lr: 8.0000e-05\n",
      "Epoch 12/20\n",
      "2717/2717 [==============================] - 186s 68ms/step - loss: 0.2236 - auc: 0.8349 - val_loss: 0.2272 - val_auc: 0.7717 - lr: 8.0000e-05\n",
      "5531/5531 [==============================] - 23s 4ms/step\n",
      "                             Pred 0                 Pred 1\n",
      "True 0  TN = 169421 (TNR = 100.00%)   FP = 7 (FPR = 0.00%)\n",
      "True 1     FN = 7531 (FNR = 99.84%)  TP = 12 (TPR = 0.16%)\n",
      "Epoch 1/20\n",
      "2717/2717 [==============================] - 186s 68ms/step - loss: 0.6340 - auc: 0.7342 - val_loss: 0.2296 - val_auc: 0.7513 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2717/2717 [==============================] - 186s 68ms/step - loss: 0.2359 - auc: 0.8161 - val_loss: 0.2291 - val_auc: 0.7407 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "2717/2717 [==============================] - 185s 68ms/step - loss: 0.2328 - auc: 0.8222 - val_loss: 0.2320 - val_auc: 0.7755 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "2717/2717 [==============================] - 188s 69ms/step - loss: 0.2312 - auc: 0.8233 - val_loss: 0.2254 - val_auc: 0.7676 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "2717/2717 [==============================] - 185s 68ms/step - loss: 0.2312 - auc: 0.8232 - val_loss: 0.2202 - val_auc: 0.7693 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "2717/2717 [==============================] - 186s 69ms/step - loss: 0.2297 - auc: 0.8254 - val_loss: 0.2155 - val_auc: 0.7744 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "2717/2717 [==============================] - 188s 69ms/step - loss: 0.2292 - auc: 0.8253 - val_loss: 0.2154 - val_auc: 0.7702 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "2717/2717 [==============================] - 185s 68ms/step - loss: 0.2277 - auc: 0.8275 - val_loss: 0.2276 - val_auc: 0.7641 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "2717/2717 [==============================] - 187s 69ms/step - loss: 0.2266 - auc: 0.8290 - val_loss: 0.2237 - val_auc: 0.7669 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "2717/2717 [==============================] - 183s 67ms/step - loss: 0.2248 - auc: 0.8325 - val_loss: 0.2260 - val_auc: 0.7693 - lr: 8.0000e-05\n",
      "Epoch 11/20\n",
      "2717/2717 [==============================] - 187s 69ms/step - loss: 0.2245 - auc: 0.8328 - val_loss: 0.2398 - val_auc: 0.7613 - lr: 8.0000e-05\n",
      "Epoch 12/20\n",
      "2717/2717 [==============================] - 186s 69ms/step - loss: 0.2246 - auc: 0.8334 - val_loss: 0.2296 - val_auc: 0.7663 - lr: 8.0000e-05\n",
      "5531/5531 [==============================] - 26s 5ms/step\n",
      "                             Pred 0                 Pred 1\n",
      "True 0  TN = 169424 (TNR = 100.00%)   FP = 4 (FPR = 0.00%)\n",
      "True 1     FN = 7527 (FNR = 99.79%)  TP = 16 (TPR = 0.21%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import talib\n",
    "import matplotlib\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend\n",
    "import psutil\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    " \n",
    "time_slot_front = 1 # minutes\n",
    "time_slot_wave = 1 # minutes\n",
    "query_condition = 'volume > 250'  # 400  100\n",
    "query_condition2 = 'volume > 100'  # 100   25 BNB\n",
    "expected_percentage = 0.5\n",
    " \n",
    "path_imp = 'C:/data_raw/train/*.csv'\n",
    "path_exp = 'C:/data_pro/'\n",
    "x_train_name = \"train_x.npy\"\n",
    "y_train_name = \"train_y.npy\"\n",
    "x_test_name = \"test_x.npy\"\n",
    "y_test_name = \"test_y.npy\"\n",
    "\n",
    "\n",
    "train_list = ['2022-03','2022-04', '2022-05', '2022-06']    \n",
    "test_list= ['2022-07']\n",
    "\n",
    "train_list_files = []\n",
    "test_list_files=[]\n",
    "for m in train_list:\n",
    "    train_list_files.append(f'C:/data_raw/ETHUSDT-aggTrades-{m}.csv.parquet')\n",
    "for m in test_list:\n",
    "    test_list_files.append(f'C:/data_raw/ETHUSDT-aggTrades-{m}.csv.parquet')\n",
    "\n",
    "\n",
    "features = 11\n",
    "events = 61\n",
    "start_time = time.time()\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "\n",
    "def telegram(message):\n",
    "    y = requests.post(f'http://api.telegram.org/bot1434154647:AAGf-SNF84zR7jQ7sGShQtJX9Jq5UurTKJI/sendMessage?chat_id=-379051501&text={message}')\n",
    "\n",
    "def conf_matrix(y,pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, normalize='true')\n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "                        index=['True 0', 'True 1'], columns=['Pred 0','Pred 1'])\n",
    "\n",
    "\n",
    "def prepare_data(read_files,mem):\n",
    "    counter = 0\n",
    "    d3 = np.zeros(shape=(mem,events, features,1), dtype=float)  # float 64 #d3 = np.zeros(shape=(mem,events, features)).astype('float32')  # float 32\n",
    "    d1 = []\n",
    "    for f in read_files:\n",
    "        print('loading to memory',f,datetime.now())\n",
    "        df_ini= pd.read_parquet(f)\n",
    "        \n",
    "        df_ini['Timestamp']=df_ini['Timestamp']//1000\n",
    "        df_01 = df_ini.loc[df_ini['is_maker'] == True].copy()\n",
    "        df_02 = df_01.groupby(\n",
    "            ['Timestamp'],as_index=False).agg(\n",
    "            C1=('Timestamp', 'count'),\n",
    "            S1=('volume', 'sum'))\n",
    "\n",
    "        df_11 = df_ini.loc[df_ini['is_maker'] == False].copy()\n",
    "        df_12 = df_11.groupby(\n",
    "            ['Timestamp'],as_index=False).agg(\n",
    "            C2=('Timestamp', 'count'),\n",
    "            S2=('volume', 'sum'))\n",
    "        df_12 = df_12.assign(Timestamp = pd.to_datetime(df_12['Timestamp'],unit='s')).set_index('Timestamp')\n",
    "        df_02 = df_02.assign(Timestamp = pd.to_datetime(df_02['Timestamp'],unit='s')).set_index('Timestamp')\n",
    "        df_ini = df_ini.assign(Timestamp = pd.to_datetime(df_ini['Timestamp'],unit='s')).set_index('Timestamp')\n",
    "\n",
    "        df_ini.drop(['tradeid','first_tradeid','last_tradeid','is_maker'], axis=1, inplace=True)\n",
    "        df_ohlc= df_ini.resample('1S')['price'].ohlc()    #1S or 1min\n",
    "        df_vol= df_ini.resample('1S')['volume'].sum()\n",
    "        df_vol2= df_ini.resample('1S')['volume'].ohlc()\n",
    "        df_vol2 = df_vol2.rename({'open': 'openv', 'close': 'closev', 'high': 'highv', 'low': 'lowv'}, axis=1)  # new method\n",
    "        df = pd.concat([df_ohlc,df_vol,df_vol2,df_12,df_02], axis=1)\n",
    "\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df=df.fillna(method='ffill')\n",
    "    \n",
    "        df['rsi'] = talib.RSI(df.close,timeperiod=100)\n",
    "        df['atr'] = talib.ATR(df.high, df.low, df.close)\n",
    "        df['macd'] = talib.MACD(df.close)[1]\n",
    "        \n",
    "        \n",
    "        df['returns'] = df.close.pct_change()*1000                     \n",
    "\n",
    " \n",
    "        df = df.iloc[1800:,]          # delete first seconds without rsi\n",
    "        df_unchanged = df.copy()\n",
    "\n",
    "        df_base= df_unchanged.query(query_condition)          # cycle through only where expected volume (unchanged - no normalizarion)\n",
    "        for index, row in df_base.iterrows():\n",
    "\n",
    "            df_front =  df_unchanged.loc[index - (time_slot_front*60 ) : index].copy() \n",
    "            df_wave = df_unchanged.loc[index : index + time_slot_wave*60  -2].copy()  \n",
    "\n",
    "            df_prefront = df_unchanged.loc[index - (time_slot_front*1800 ) : index].copy()\n",
    "            my_mean = df_prefront['close'].mean()\n",
    "            df_front['to_mean'] = df_front['close']/my_mean*100\n",
    "            my_largest = df_prefront['close'].nlargest(3).mean()\n",
    "            df_front['to_largest'] = df_front['close']/my_largest*100\n",
    "            my_smallest = df_prefront['close'].nsmallest(3).mean()\n",
    "            df_front['to_smallest'] = df_front['close']/my_smallest*100\n",
    "            \n",
    "            \n",
    "            if len(df_front.index) == (time_slot_front*60 + 1) and len (df_wave.index)== (time_slot_wave*60 + 1-2):    \n",
    "                max_price = df_wave['close'].max()\n",
    "                min_price = df_wave['close'].min()\n",
    "                initial_price = df_wave.iloc[0]['close']\n",
    "                element_result = 0\n",
    "                if max_price >= initial_price * 1.005:\n",
    "                    try:\n",
    "                        deal_index_pos = df_wave[df_wave['close'].gt(initial_price*1.005)].index[0]\n",
    "                    except:\n",
    "                        deal_index_pos = 0\n",
    "                    try:\n",
    "                        deal_index_neg = df_wave[df_wave['close'].lt(initial_price*0.995)].index[0]\n",
    "                    except:\n",
    "                        deal_index_neg = 0\n",
    "                    if deal_index_pos < deal_index_neg: element_result = 1\n",
    "                    if deal_index_neg == 0 and deal_index_pos >0: element_result = 1\n",
    "                    \n",
    "                    \n",
    "                df_front.drop(['open','high','low','openv','highv','lowv','closev','volume','close'], axis=1, inplace=True)\n",
    " \n",
    "                #df_front.drop(['S1','rsi','macd','to_mean','S2','returns'], axis=1, inplace=True)\n",
    "\n",
    "                element_numpy = df_front.to_numpy()\n",
    "                d3[counter] = element_numpy.reshape(1, events, features,1)  # оптимально добавляем с увеличением размерности массива до 3d\n",
    "                d1.append(element_result)   #просто добавляем элемент массива\n",
    "                counter += 1\n",
    "                if counter % 15000 ==0: print(f, counter,\" frames per minute =\" , round(counter*60/((time.time() - start_time)), 1))\n",
    "    d1 = np.array(d1)\n",
    "    d3= d3[:counter]\n",
    "    print('final d3', d3.shape[0],'positive',np.sum(d1))\n",
    "    print(list(df_front))\n",
    "    return d3,d1\n",
    "\n",
    "def conf_matrix(y,pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, normalize='true')\n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "                        index=['True 0', 'True 1'], columns=['Pred 0','Pred 1'])\n",
    "\n",
    "\n",
    "\n",
    "x,y = prepare_data(train_list_files,2000000)\n",
    "\n",
    "np.save (x_train_name,x)\n",
    "np.save (y_train_name,y)\n",
    "\n",
    "_x,_y = prepare_data(test_list_files,900000)\n",
    "\n",
    "np.save (x_test_name,_x)\n",
    "np.save (y_test_name,_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "if 'model' in globals():del model\n",
    "\n",
    "\n",
    "for period in range(2):\n",
    "\n",
    "    backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size = (4), activation='relu',input_shape=[events, features,1]))\n",
    "    #model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(2048, activation='relu',activity_regularizer= tf.keras.regularizers.L1(l1=0.01))) \n",
    "    model.add(Dense(2048, activation='relu')) \n",
    "    #model.add(Dropout(0.3))                             \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='rmsprop',\n",
    "                   metrics=['AUC'])\n",
    "    backend.set_value(model.optimizer.learning_rate,0.0001)\n",
    "    callback_list=[\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(factor=0.8,monitor=\"val_loss\",patience=3)]\n",
    "    history = model.fit(x = x, y = y,  epochs=20,   validation_split=0.3,   batch_size= 70,callbacks=callback_list)\n",
    "    model.save(f'opt_exp_21_{str(period)}.h5')\n",
    "\n",
    "    predictions = model.predict(_x)  \n",
    "    \n",
    "    pred_tuned_thresh = np.where(predictions >= 0.75, 1, 0) \n",
    "    print(conf_matrix(_y,pred_tuned_thresh))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can continue with uploaded arrays and change hyperparamenters\n",
    "\n",
    "x = np.load (x_train_name)\n",
    "y = np.load (y_train_name)\n",
    "\n",
    "\n",
    "_x = np.load (x_test_name)\n",
    "_y = np.load (y_test_name)\n",
    "\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "if 'model' in globals():del model\n",
    "\n",
    "\n",
    "for period in range(2):\n",
    "\n",
    "    backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size = (4), activation='relu',input_shape=[events, features,1]))\n",
    "    #model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(2048, activation='relu',activity_regularizer= tf.keras.regularizers.L1(l1=0.01))) \n",
    "    model.add(Dense(2048, activation='relu')) \n",
    "    #model.add(Dropout(0.3))                             \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='rmsprop',\n",
    "                   metrics=['AUC'])\n",
    "    backend.set_value(model.optimizer.learning_rate,0.0001)\n",
    "    callback_list=[\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(factor=0.8,monitor=\"val_loss\",patience=3)]\n",
    "    history = model.fit(x = x, y = y,  epochs=20,   validation_split=0.3,   batch_size= 70,callbacks=callback_list)\n",
    "    model.save(f'opt_exp_21_{str(period)}.h5')\n",
    "\n",
    "    predictions = model.predict(_x)  \n",
    "    \n",
    "    pred_tuned_thresh = np.where(predictions >= 0.75, 1, 0) \n",
    "    print(conf_matrix(_y,pred_tuned_thresh))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare all coins CLOSE by seconds - we need FACTOR12\n",
    "\n",
    "import requests\n",
    "import zipfile, io\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "periods = [ '2022-01', '2022-02', '2022-03', '2022-04','2022-05', '2022-06','2022-07','2022-08', '2022-09', '2022-10']  \n",
    "coins = ['BCH','BNB','BTC','EOS','ETC','ETH','LTC','XMR','TRX','XLM','ADA','IOTA','MKR','DOGE']\n",
    "\n",
    "#1\n",
    "\n",
    "for period in periods:\n",
    "    for coin in coins:\n",
    "        coin_usd = coin + 'USDT'\n",
    "        zip_file_url = f'https://data.binance.vision/data/futures/um/monthly/aggTrades/{coin_usd}/{coin_usd}-aggTrades-{period}.zip'\n",
    "        print('Uploading & unpacking ',zip_file_url)\n",
    "        try:\n",
    "            r = requests.get(zip_file_url)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(\"E:/feat12\")\n",
    "        except:\n",
    "            print('no data')\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "read_files = glob.glob(f\"E:/feat12/*.csv\")\n",
    "for f in read_files:\n",
    "    df= pd.read_csv(f,sep=',',header=None,  skiprows=1,nrows=999999999,\n",
    "                    names=['tradeid','price','volume','first_tradeid','last_tradeid','Timestamp','is_maker'])\n",
    "    df['Timestamp']=df['Timestamp']//1000\n",
    "    df.drop(['tradeid','first_tradeid','last_tradeid','is_maker'], axis=1, inplace=True)\n",
    "    df = df.assign(Timestamp = pd.to_datetime(df['Timestamp'],unit='s')).set_index('Timestamp')\n",
    "    df_sampled= df.resample('1S')['price'].ohlc()\n",
    "    df_sampled=df_sampled.fillna(method='ffill')\n",
    "    df_sampled.drop(['open','high','low'], axis=1, inplace=True)\n",
    "    df_sampled.to_parquet(f+'.parquet') \n",
    "    print (f+'.parquet')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for coin in coins:\n",
    "    read_files = glob.glob(f\"E:/feat12/{coin}*.parquet\")\n",
    "    first_run = 1\n",
    "    df = pd.DataFrame()\n",
    "    for f in read_files:\n",
    "        curr_column = f.split('USDT-aggTrades-')[0].split('\\\\')[1]\n",
    "        print (f)\n",
    "        df_ini = pd.read_parquet(f)\n",
    "        df_ini  = df_ini.rename(columns={'close': curr_column})\n",
    "        if first_run ==1:\n",
    "            first_run = 0\n",
    "            df = df_ini\n",
    "            print ('A 0 - just copy')\n",
    "        else:    \n",
    "            print ('A 1 - axis=0')        \n",
    "            df = pd.concat([df,df_ini], axis=0)\n",
    "    \n",
    "    df.to_parquet(f'E:/feat12/coins/{coin}.parquet')\n",
    "\n",
    "    \n",
    "df = pd.DataFrame()    \n",
    "read_files = glob.glob(f\"E:/feat12/coins/*.parquet\") \n",
    "first_run = 1\n",
    "for f in read_files:\n",
    "    df_ini = pd.read_parquet(f)\n",
    "    if len(df_ini) > 0 :\n",
    "        if first_run ==1:\n",
    "            df = df_ini\n",
    "            first_run = 0\n",
    "            print ('B 0 - just copy')\n",
    "        else:\n",
    "            print ('B 2 - axis=1')        \n",
    "            df = pd.concat([df,df_ini], axis=1)\n",
    "        \n",
    "df.to_parquet('E:/feat12/full/full.parquet')        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58460dd5",
   "metadata": {},
   "source": [
    "# 2. Base model + FACTOR12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca8cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-03.csv.parquet 2022-12-20 16:21:16.055503\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-03.csv.parquet 15000  frames per minute = 11943.3\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-03.csv.parquet 30000  frames per minute = 14161.4\n",
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-04.csv.parquet 2022-12-20 16:23:24.578199\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-04.csv.parquet 45000  frames per minute = 14253.1\n",
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 2022-12-20 16:25:00.899004\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 60000  frames per minute = 14052.5\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 75000  frames per minute = 14633.6\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 90000  frames per minute = 15047.9\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-05.csv.parquet 105000  frames per minute = 15343.2\n",
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 2022-12-20 16:28:14.930667\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 120000  frames per minute = 14871.9\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 135000  frames per minute = 15147.0\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 150000  frames per minute = 15357.3\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 165000  frames per minute = 15532.6\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 180000  frames per minute = 15649.9\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 195000  frames per minute = 15819.8\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 210000  frames per minute = 15964.2\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 225000  frames per minute = 16098.3\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 240000  frames per minute = 16225.0\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 255000  frames per minute = 16146.7\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-06.csv.parquet 270000  frames per minute = 16198.8\n",
      "final d3 271626 positive 20658\n",
      "['open', 'high', 'low', 'close', 'volume', 'openv', 'highv', 'lowv', 'closev', 'C2', 'S2', 'C1', 'S1', 'f012', 'f012_max_60', 'f012_min_60', 'f012_max_300', 'f012_min_300', 'f012_max_900', 'f012_min_900', 'f012_max_1800', 'f012_min_1800', 'rsi', 'atr', 'macd', 'returns', 'to_mean', 'to_largest', 'to_smallest']\n",
      "loading to memory C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 2022-12-20 16:38:04.324662\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 15000  frames per minute = 827.3\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 30000  frames per minute = 1580.7\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 45000  frames per minute = 2271.3\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 60000  frames per minute = 2905.1\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 75000  frames per minute = 3488.6\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 90000  frames per minute = 4023.2\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 105000  frames per minute = 4514.9\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 120000  frames per minute = 4969.4\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 135000  frames per minute = 5399.2\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 150000  frames per minute = 5797.6\n",
      "C:/data_raw/ETHUSDT-aggTrades-2022-07.csv.parquet 165000  frames per minute = 6169.3\n",
      "final d3 176971 positive 7543\n",
      "['open', 'high', 'low', 'close', 'volume', 'openv', 'highv', 'lowv', 'closev', 'C2', 'S2', 'C1', 'S1', 'f012', 'f012_max_60', 'f012_min_60', 'f012_max_300', 'f012_min_300', 'f012_max_900', 'f012_min_900', 'f012_max_1800', 'f012_min_1800', 'rsi', 'atr', 'macd', 'returns', 'to_mean', 'to_largest', 'to_smallest']\n",
      "Epoch 1/20\n",
      "2717/2717 [==============================] - 326s 120ms/step - loss: 0.3262 - auc: 0.4995 - val_loss: 0.2353 - val_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2717/2717 [==============================] - 326s 120ms/step - loss: 0.2834 - auc: 0.5023 - val_loss: 0.2366 - val_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      " 107/2717 [>.............................] - ETA: 4:45 - loss: 0.2911 - auc: 0.4825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import talib\n",
    "import matplotlib\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend\n",
    "import psutil\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    " \n",
    "time_slot_front = 1 # minutes\n",
    "time_slot_wave = 1 # minutes\n",
    "query_condition = 'volume > 250'  # 400  100\n",
    "query_condition2 = 'volume > 100'  # 100   25 BNB\n",
    "expected_percentage = 0.5\n",
    " \n",
    "path_imp = 'C:/data_raw/train/*.csv'\n",
    "path_exp = 'C:/data_pro/'\n",
    "x_train_name = \"train_x.npy\"\n",
    "y_train_name = \"train_y.npy\"\n",
    "x_test_name = \"test_x.npy\"\n",
    "y_test_name = \"test_y.npy\"\n",
    "\n",
    "\n",
    "train_list = ['2022-03','2022-04', '2022-05', '2022-06']    \n",
    "test_list= ['2022-07']\n",
    "\n",
    "train_list_files = []\n",
    "test_list_files=[]\n",
    "for m in train_list:\n",
    "    train_list_files.append(f'C:/data_raw/ETHUSDT-aggTrades-{m}.csv.parquet')\n",
    "for m in test_list:\n",
    "    test_list_files.append(f'C:/data_raw/ETHUSDT-aggTrades-{m}.csv.parquet')\n",
    "\n",
    "\n",
    "features = 20\n",
    "events = 61\n",
    "start_time = time.time()\n",
    "\n",
    "# insertion 1\n",
    "full_close = pd.read_parquet ('C:/data_raw/full/full.parquet')\n",
    "full_close=full_close.fillna(method='ffill')\n",
    "full_close = full_close.dropna()\n",
    "#\n",
    "\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "\n",
    "def telegram(message):\n",
    "    y = requests.post(f'http://api.telegram.org/bot1434154647:AAGf-SNF84zR7jQ7sGShQtJX9Jq5UurTKJI/sendMessage?chat_id=-379051501&text={message}')\n",
    "\n",
    "def conf_matrix(y,pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, normalize='true')\n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "                        index=['True 0', 'True 1'], columns=['Pred 0','Pred 1'])\n",
    "\n",
    "\n",
    "def prepare_data(read_files,mem):\n",
    "    counter = 0\n",
    "    d3 = np.zeros(shape=(mem,events, features,1), dtype=float)  # float 64 #d3 = np.zeros(shape=(mem,events, features)).astype('float32')  # float 32\n",
    "    d1 = []\n",
    "    for f in read_files:\n",
    "        print('loading to memory',f,datetime.now())\n",
    "        df_ini= pd.read_parquet(f)\n",
    "        \n",
    "        df_ini['Timestamp']=df_ini['Timestamp']//1000\n",
    "        df_01 = df_ini.loc[df_ini['is_maker'] == True].copy()\n",
    "        df_02 = df_01.groupby(\n",
    "            ['Timestamp'],as_index=False).agg(\n",
    "            C1=('Timestamp', 'count'),\n",
    "            S1=('volume', 'sum'))\n",
    "\n",
    "        df_11 = df_ini.loc[df_ini['is_maker'] == False].copy()\n",
    "        df_12 = df_11.groupby(\n",
    "            ['Timestamp'],as_index=False).agg(\n",
    "            C2=('Timestamp', 'count'),\n",
    "            S2=('volume', 'sum'))\n",
    "        df_12 = df_12.assign(Timestamp = pd.to_datetime(df_12['Timestamp'],unit='s')).set_index('Timestamp')\n",
    "        df_02 = df_02.assign(Timestamp = pd.to_datetime(df_02['Timestamp'],unit='s')).set_index('Timestamp')\n",
    "        df_ini = df_ini.assign(Timestamp = pd.to_datetime(df_ini['Timestamp'],unit='s')).set_index('Timestamp')\n",
    "\n",
    "        df_ini.drop(['tradeid','first_tradeid','last_tradeid','is_maker'], axis=1, inplace=True)\n",
    "        df_ohlc= df_ini.resample('1S')['price'].ohlc()    #1S or 1min\n",
    "        df_vol= df_ini.resample('1S')['volume'].sum()\n",
    "        df_vol2= df_ini.resample('1S')['volume'].ohlc()\n",
    "        df_vol2 = df_vol2.rename({'open': 'openv', 'close': 'closev', 'high': 'highv', 'low': 'lowv'}, axis=1)  # new method\n",
    "        df = pd.concat([df_ohlc,df_vol,df_vol2,df_12,df_02], axis=1)\n",
    "\n",
    "        # insertion 2  (factor 12)\n",
    "        df =  pd.concat ([df,full_close],axis=1 ,join='inner' )\n",
    "        df['f012'] = df['close']/(df['BCH'] + df['BNB'] + df['BTC'] + df['EOS'] + df['ETC']  + df['ETH'] + \n",
    "                                  df['LTC'] + df['XMR'] + df['TRX'] + df['XLM'] + df['ADA'] + df['IOTA'] + df['MKR']  + df['DOGE'])  \n",
    "        lags = [60,300,900,1800]\n",
    "        for lag in lags:\n",
    "            df[f'f012_max_{lag}'] = df['f012'].rolling(lag).max()\n",
    "            df[f'f012_min_{lag}'] = df['f012'].rolling(lag).min()\n",
    "\n",
    "        df.drop(['BCH','BNB','BTC','EOS','ETC','ETH','LTC','XMR','TRX','XLM','ADA','IOTA','MKR','DOGE'], axis=1, inplace=True)\n",
    "        #\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df=df.fillna(method='ffill')\n",
    "    \n",
    "        df['rsi'] = talib.RSI(df.close,timeperiod=100)\n",
    "        df['atr'] = talib.ATR(df.high, df.low, df.close)\n",
    "        df['macd'] = talib.MACD(df.close)[1]\n",
    "        \n",
    "        \n",
    "        df['returns'] = df.close.pct_change()*1000                     \n",
    "\n",
    " \n",
    "        df = df.iloc[1800:,]          # delete first seconds without rsi\n",
    "        df_unchanged = df.copy()\n",
    "\n",
    "        df_base= df_unchanged.query(query_condition)          # cycle through only where expected volume (unchanged - no normalizarion)\n",
    "        for index, row in df_base.iterrows():\n",
    "\n",
    "            df_front =  df_unchanged.loc[index - (time_slot_front*60 ) : index].copy() \n",
    "            df_wave = df_unchanged.loc[index : index + time_slot_wave*60  -2].copy()  \n",
    "\n",
    "            df_prefront = df_unchanged.loc[index - (time_slot_front*1800 ) : index].copy()\n",
    "            my_mean = df_prefront['close'].mean()\n",
    "            df_front['to_mean'] = df_front['close']/my_mean*100\n",
    "            my_largest = df_prefront['close'].nlargest(3).mean()\n",
    "            df_front['to_largest'] = df_front['close']/my_largest*100\n",
    "            my_smallest = df_prefront['close'].nsmallest(3).mean()\n",
    "            df_front['to_smallest'] = df_front['close']/my_smallest*100\n",
    "            \n",
    "            \n",
    "            if len(df_front.index) == (time_slot_front*60 + 1) and len (df_wave.index)== (time_slot_wave*60 + 1-2):    \n",
    "                max_price = df_wave['close'].max()\n",
    "                min_price = df_wave['close'].min()\n",
    "                initial_price = df_wave.iloc[0]['close']\n",
    "                element_result = 0\n",
    "                if max_price >= initial_price * 1.005:\n",
    "                    try:\n",
    "                        deal_index_pos = df_wave[df_wave['close'].gt(initial_price*1.005)].index[0]\n",
    "                    except:\n",
    "                        deal_index_pos = 0\n",
    "                    try:\n",
    "                        deal_index_neg = df_wave[df_wave['close'].lt(initial_price*0.995)].index[0]\n",
    "                    except:\n",
    "                        deal_index_neg = 0\n",
    "                    if deal_index_pos < deal_index_neg: element_result = 1\n",
    "                    if deal_index_neg == 0 and deal_index_pos >0: element_result = 1\n",
    "                    \n",
    "                    \n",
    "                df_front.drop(['open','high','low','openv','highv','lowv','closev','volume','close'], axis=1, inplace=True)\n",
    " \n",
    "                #df_front.drop(['S1','rsi','macd','to_mean','S2','returns'], axis=1, inplace=True)\n",
    "\n",
    "                element_numpy = df_front.to_numpy()\n",
    "                d3[counter] = element_numpy.reshape(1, events, features,1)  # оптимально добавляем с увеличением размерности массива до 3d\n",
    "                d1.append(element_result)   #просто добавляем элемент массива\n",
    "                counter += 1\n",
    "                if counter % 15000 ==0: print(f, counter,\" frames per minute =\" , round(counter*60/((time.time() - start_time)), 1))\n",
    "    d1 = np.array(d1)\n",
    "    d3= d3[:counter]\n",
    "    print('final d3', d3.shape[0],'positive',np.sum(d1))\n",
    "    print(list(df_front))\n",
    "    return d3,d1\n",
    "\n",
    "def conf_matrix(y,pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, normalize='true')\n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "                        index=['True 0', 'True 1'], columns=['Pred 0','Pred 1'])\n",
    "\n",
    "x,y = prepare_data(train_list_files,2000000)\n",
    "np.save (x_train_name,x)\n",
    "np.save (y_train_name,y)\n",
    "\n",
    "_x,_y = prepare_data(test_list_files,900000)\n",
    "np.save (x_test_name,_x)\n",
    "np.save (y_test_name,_y)\n",
    "\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "if 'model' in globals():del model\n",
    "\n",
    "for period in range(2):\n",
    "    backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size = (4), activation='relu',input_shape=[events, features,1]))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation='relu')) \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='rmsprop',\n",
    "                   metrics=['AUC'])\n",
    "    backend.set_value(model.optimizer.learning_rate,0.0001)\n",
    "    callback_list=[\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(factor=0.8,monitor=\"val_loss\",patience=3)]\n",
    "    history = model.fit(x = x, y = y,  epochs=20,   validation_split=0.3,   batch_size= 70,callbacks=callback_list)\n",
    "    model.save(f'opt_exp_21_{str(period)}.h5')\n",
    "\n",
    "    predictions = model.predict(_x)  \n",
    "    \n",
    "    pred_tuned_thresh = np.where(predictions >= 0.75, 1, 0) \n",
    "    print(conf_matrix(_y,pred_tuned_thresh))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e18c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5531/5531 [==============================] - 34s 6ms/step\n",
      "                             Pred 0                Pred 1\n",
      "True 0  TN = 169428 (TNR = 100.00%)  FP = 0 (FPR = 0.00%)\n",
      "True 1    FN = 7543 (FNR = 100.00%)  TP = 0 (TPR = 0.00%)\n",
      "5531/5531 [==============================] - 34s 6ms/step\n",
      "                             Pred 0                Pred 1\n",
      "True 0  TN = 169428 (TNR = 100.00%)  FP = 0 (FPR = 0.00%)\n",
      "True 1    FN = 7543 (FNR = 100.00%)  TP = 0 (TPR = 0.00%)\n"
     ]
    }
   ],
   "source": [
    "# we can continue with uploaded arrays and change hyperparamenters\n",
    "\n",
    "x = np.load (x_train_name)\n",
    "y = np.load (y_train_name)\n",
    "\n",
    "\n",
    "_x = np.load (x_test_name)\n",
    "_y = np.load (y_test_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for period in range(2):\n",
    "    '''\n",
    "    backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size = (4), activation='relu',input_shape=[events, features,1]))\n",
    "    #model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(2048, activation='relu',activity_regularizer= tf.keras.regularizers.L1(l1=0.01))) \n",
    "    model.add(Dense(2048, activation='relu')) \n",
    "    #model.add(Dropout(0.3))                             \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='rmsprop',\n",
    "                   metrics=['AUC'])\n",
    "    backend.set_value(model.optimizer.learning_rate,0.0001)\n",
    "    callback_list=[\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(factor=0.8,monitor=\"val_loss\",patience=3)]\n",
    "    history = model.fit(x = x, y = y,  epochs=20,   validation_split=0.3,   batch_size= 70,callbacks=callback_list)\n",
    "    model.save(f'opt_exp_21_{str(period)}.h5')\n",
    "    '''\n",
    "\n",
    "    predictions = model.predict(_x)  \n",
    "    \n",
    "    pred_tuned_thresh = np.where(predictions >= 0.75, 1, 0) \n",
    "    print(conf_matrix(_y,pred_tuned_thresh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192f4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
